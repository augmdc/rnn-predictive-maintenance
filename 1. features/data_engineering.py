"""
Data Engineering Module for NASA C-MAPSS Turbofan Engine Degradation Dataset.

This module provides comprehensive data preprocessing for Remaining Useful Life (RUL) 
prediction with MEMORY-EFFICIENT implementations:

Key Optimizations:
- NumPy stride tricks for zero-copy sliding windows (no RAM spike)
- Fully vectorized operations (no Python for-loops over engines)
- Typed NumPy arrays instead of Python lists/dicts for performance
- Handles 100,000+ rows with sequence_length=50 efficiently

Dataset Description:
- Each dataset contains run-to-failure time series from multiple engines
- 26 columns: unit_id, time_cycles, 3 operational settings, 21 sensor measurements
- Training data: full trajectories until failure
- Test data: truncated trajectories (predict RUL at last cycle)
- RUL files: ground truth RUL for test data
"""

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from pathlib import Path
from typing import Tuple, Optional, Literal
from numpy.typing import NDArray


# =============================================================================
# Path Configuration
# =============================================================================

SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
DATA_DIR = PROJECT_ROOT / 'data'


# =============================================================================
# Column Definitions (Using NumPy string arrays for efficiency)
# =============================================================================

# Full descriptive column names based on C-MAPSS documentation
# The 26 columns are: unit_id, time, 3 operational settings, 21 sensor measurements
COLUMN_NAMES: NDArray[np.str_] = np.array([
    'unit_id',                    # Engine unit identifier
    'time_cycles',                # Operating cycle index
    'op_setting_1',               # Operational setting 1 (Altitude)
    'op_setting_2',               # Operational setting 2 (Mach Number)
    'op_setting_3',               # Operational setting 3 (Throttle Resolver Angle)
    'T2',                         # Total temperature at fan inlet (Â°R)
    'T24',                        # Total temperature at LPC outlet (Â°R)
    'T30',                        # Total temperature at HPC outlet (Â°R)
    'T50',                        # Total temperature at LPT outlet (Â°R)
    'P2',                         # Pressure at fan inlet (psia)
    'P15',                        # Total pressure in bypass-duct (psia)
    'P30',                        # Total pressure at HPC outlet (psia)
    'Ps30',                       # Static pressure at HPC outlet (psia)
    'phi',                        # Ratio of fuel flow to Ps30 (pps/psi)
    'NRf',                        # Corrected fan speed (rpm)
    'NRc',                        # Corrected core speed (rpm)
    'epr',                        # Engine pressure ratio (P50/P2)
    'Nc',                         # Physical core speed (rpm)
    'Nf',                         # Physical fan speed (rpm)
    'BPR',                        # Bypass Ratio
    'farB',                       # Burner fuel-air ratio
    'htBleed',                    # Bleed Enthalpy
    'Nf_dmd',                     # Demanded fan speed (rpm)
    'PCNfR_dmd',                  # Demanded corrected fan speed (rpm)
    'W31',                        # HPT coolant bleed (lbm/s)
    'W32',                        # LPT coolant bleed (lbm/s)
], dtype='U20')

# Column indices for fast array-based access
ID_COL_INDICES: NDArray[np.int32] = np.array([0, 1], dtype=np.int32)
SETTING_COL_INDICES: NDArray[np.int32] = np.array([2, 3, 4], dtype=np.int32)
SENSOR_COL_INDICES: NDArray[np.int32] = np.arange(5, 26, dtype=np.int32)

# Column groupings (as tuples for immutability, converted to arrays when needed)
ID_COLUMNS: tuple = ('unit_id', 'time_cycles')
SETTING_COLUMNS: tuple = ('op_setting_1', 'op_setting_2', 'op_setting_3')
SENSOR_COLUMNS: tuple = (
    'T2', 'T24', 'T30', 'T50',           # Temperature sensors
    'P2', 'P15', 'P30', 'Ps30',          # Pressure sensors
    'phi',                                # Fuel flow ratio
    'NRf', 'NRc',                         # Corrected speeds
    'epr',                                # Engine pressure ratio
    'Nc', 'Nf',                           # Physical speeds
    'BPR', 'farB', 'htBleed',             # Ratios and bleed
    'Nf_dmd', 'PCNfR_dmd',                # Demanded speeds
    'W31', 'W32',                         # Coolant bleed flows
)

# NOTE: We do NOT hardcode which sensors to drop. Instead, we compute this
# dynamically based on variance analysis of each dataset. Sensors with 
# near-zero variance provide no discriminative information for RUL prediction.
# Use `analyze_sensor_variance()` to determine which sensors to drop.


# =============================================================================
# Data Loading Functions
# =============================================================================

def load_dataset(
    dataset_id: Literal['FD001', 'FD002', 'FD003', 'FD004'],
    data_dir: Path = DATA_DIR
) -> Tuple[pd.DataFrame, pd.DataFrame, NDArray[np.float32]]:
    """
    Load dataset with data in CORRECT FORMAT from the start.
    
    Ensures all type conversions happen during load, not later in hot paths.
    Uses dtype specifications in pd.read_csv for efficient loading.
    """
    # PRE-DEFINE: Column dtypes for efficient loading
    # This ensures data is loaded in correct format, avoiding later conversions
    dtype_dict = {
        'unit_id': np.int32,
        'time_cycles': np.int32,
    }
    # All other columns are float32
    for col in COLUMN_NAMES[2:]:
        dtype_dict[col] = np.float32
    
    # Load with correct dtypes from the start
    train_df = pd.read_csv(
        data_dir / f'train_{dataset_id}.txt',
        sep=r'\s+', 
        header=None, 
        names=COLUMN_NAMES.tolist(),
        dtype=dtype_dict,
        engine='c'  # Use faster C engine
    )
    
    test_df = pd.read_csv(
        data_dir / f'test_{dataset_id}.txt',
        sep=r'\s+', 
        header=None, 
        names=COLUMN_NAMES.tolist(),
        dtype=dtype_dict,
        engine='c'
    )
    
    # Load RUL as typed array directly
    rul_array: NDArray[np.float32] = np.loadtxt(
        data_dir / f'RUL_{dataset_id}.txt',
        dtype=np.float32
    )
    
    return train_df, test_df, rul_array


def load_all_datasets(data_dir: Path = DATA_DIR) -> dict:
    """Load all four C-MAPSS datasets."""
    dataset_ids = np.array(['FD001', 'FD002', 'FD003', 'FD004'], dtype='U5')
    return {did: load_dataset(did, data_dir) for did in dataset_ids}


# =============================================================================
# RUL Computation (Fully Vectorized with Typed Arrays)
# =============================================================================

def compute_rul_vectorized(df: pd.DataFrame, max_rul: Optional[int] = None) -> pd.DataFrame:
    """
    Compute RUL with data in correct format throughout.
    
    Uses in-place operations where possible to avoid unnecessary copies.
    """
    df = df.copy()
    
    # Extract as typed arrays (data should already be correct type from load)
    # If loaded correctly, these are zero-copy views
    unit_ids: NDArray[np.int32] = df['unit_id'].values
    time_cycles: NDArray[np.int32] = df['time_cycles'].values
    
    # Vectorized: get max cycle per engine
    max_cycles_per_unit: NDArray[np.int32] = df.groupby('unit_id')['time_cycles'].transform('max').values
    
    # Compute RUL as typed array (single allocation)
    rul: NDArray[np.float32] = (max_cycles_per_unit - time_cycles).astype(np.float32)
    
    # In-place clip using numpy (no additional allocation if max_rul provided)
    if max_rul is not None:
        np.minimum(rul, max_rul, out=rul)
    
    # Single assignment to dataframe
    df['RUL'] = rul
    return df


def add_test_rul_vectorized(test_df: pd.DataFrame, rul_array: NDArray[np.float32]) -> pd.DataFrame:
    """
    Add RUL labels with all data in correct format.
    
    Pre-allocates lookup array and uses vectorized operations throughout.
    """
    test_df = test_df.copy()
    
    # Extract typed arrays (should be zero-copy views if loaded correctly)
    unit_ids: NDArray[np.int32] = test_df['unit_id'].values
    time_cycles: NDArray[np.int32] = test_df['time_cycles'].values
    
    # PRE-ALLOCATE lookup array (faster than dict for integer keys)
    max_unit = unit_ids.max()
    rul_lookup: NDArray[np.float32] = np.zeros(max_unit + 1, dtype=np.float32)
    rul_lookup[1:len(rul_array) + 1] = rul_array
    
    # Vectorized: get max cycle per engine
    max_cycles_per_unit: NDArray[np.int32] = test_df.groupby('unit_id')['time_cycles'].transform('max').values
    
    # Vectorized lookup and compute (single pass, no intermediate arrays)
    rul_at_last: NDArray[np.float32] = rul_lookup[unit_ids]
    
    # Direct computation and assignment (minimal allocations)
    test_df['RUL'] = rul_at_last + (max_cycles_per_unit - time_cycles).astype(np.float32)
    
    return test_df


# Legacy aliases for backward compatibility
compute_rul = compute_rul_vectorized
add_test_rul = add_test_rul_vectorized


# =============================================================================
# Feature Engineering (Using Typed Arrays)
# =============================================================================

def analyze_sensor_variance(
    df: pd.DataFrame,
    variance_threshold: float = 1e-5,
    verbose: bool = True
) -> dict:
    """
    Analyze sensor variance using typed arrays for performance.
    
    Returns arrays instead of lists for downstream vectorized operations.
    """
    # Get sensor columns that exist in df as numpy array
    sensor_cols_arr: NDArray[np.str_] = np.array(
        [c for c in SENSOR_COLUMNS if c in df.columns], dtype='U20'
    )
    n_sensors = len(sensor_cols_arr)
    
    # Compute variance as typed array (vectorized)
    variances: NDArray[np.float64] = df[sensor_cols_arr.tolist()].var().values
    
    # Boolean mask for constant sensors (vectorized)
    is_constant: NDArray[np.bool_] = variances < variance_threshold
    
    # Use boolean indexing (faster than list comprehension)
    constant_sensors: NDArray[np.str_] = sensor_cols_arr[is_constant]
    informative_sensors: NDArray[np.str_] = sensor_cols_arr[~is_constant]
    
    if verbose:
        print("\nðŸ“Š Sensor Variance Analysis:")
        print("-" * 60)
        print(f"{'Sensor':<15} {'Variance':>15} {'Status':<20}")
        print("-" * 60)
        
        # Vectorized status assignment
        statuses = np.where(is_constant, "âš ï¸  CONSTANT (drop)", "âœ“ Informative")
        
        for i in range(n_sensors):
            print(f"{sensor_cols_arr[i]:<15} {variances[i]:>15.6f} {statuses[i]:<20}")
        
        print("-" * 60)
        print(f"\nSensors to DROP ({len(constant_sensors)}): {constant_sensors.tolist()}")
        print(f"Sensors to KEEP ({len(informative_sensors)}): {informative_sensors.tolist()}")
    
    return {
        'variances': variances,
        'variance_names': sensor_cols_arr,
        'constant_mask': is_constant,
        'constant_sensors': constant_sensors,
        'informative_sensors': informative_sensors,
        'drop_recommendation': constant_sensors
    }


def identify_constant_columns_mask(
    data: NDArray[np.float32],
    threshold: float = 1e-5
) -> NDArray[np.bool_]:
    """
    Identify constant columns using typed arrays and boolean mask.
    
    Returns a boolean mask (True = constant, should drop).
    Much faster than returning list of column names.
    """
    # Compute variance along axis 0 (columns)
    variances: NDArray[np.float64] = np.var(data, axis=0, dtype=np.float64)
    return variances < threshold


def select_features_fast(
    df: pd.DataFrame,
    drop_constant: bool = True,
    drop_settings: bool = False,
    variance_threshold: float = 1e-5
) -> Tuple[NDArray[np.float32], NDArray[np.float32], NDArray[np.int32], NDArray[np.str_]]:
    """
    Fast feature selection returning typed NumPy arrays directly.
    
    Bypasses DataFrame operations for maximum performance.
    
    Returns:
        Tuple of (features, targets, unit_ids, feature_names)
        All as typed NumPy arrays.
    """
    # Build feature column list
    feature_cols: NDArray[np.str_] = np.array(
        [c for c in SENSOR_COLUMNS if c in df.columns], dtype='U20'
    )
    
    if not drop_settings:
        settings_arr = np.array([c for c in SETTING_COLUMNS if c in df.columns], dtype='U20')
        feature_cols = np.concatenate([settings_arr, feature_cols])
    
    # Extract data as contiguous typed array
    features: NDArray[np.float32] = np.ascontiguousarray(
        df[feature_cols.tolist()].values, dtype=np.float32
    )
    
    # Drop constant columns using boolean mask
    if drop_constant:
        keep_mask: NDArray[np.bool_] = ~identify_constant_columns_mask(features, variance_threshold)
        features = np.ascontiguousarray(features[:, keep_mask])
        feature_cols = feature_cols[keep_mask]
    
    # Extract other arrays
    targets: NDArray[np.float32] = df['RUL'].values.astype(np.float32)
    unit_ids: NDArray[np.int32] = df['unit_id'].values.astype(np.int32)
    
    return features, targets, unit_ids, feature_cols


def select_features(
    df: pd.DataFrame,
    drop_constant: bool = True,
    drop_settings: bool = False,
    custom_drop: Optional[tuple] = None,
    variance_threshold: float = 1e-5
) -> Tuple[pd.DataFrame, tuple]:
    """
    Select features for model training with data-driven sensor filtering.
    
    Returns DataFrame for compatibility, but uses typed arrays internally.
    """
    # Build feature column array
    feature_cols_arr: NDArray[np.str_] = np.array(
        [c for c in SENSOR_COLUMNS if c in df.columns], dtype='U20'
    )
    
    if not drop_settings:
        settings_arr = np.array([c for c in SETTING_COLUMNS if c in df.columns], dtype='U20')
        feature_cols_arr = np.concatenate([settings_arr, feature_cols_arr])
    
    # Drop constant sensors using boolean mask on data
    if drop_constant:
        data = df[feature_cols_arr.tolist()].values.astype(np.float32)
        keep_mask = ~identify_constant_columns_mask(data, variance_threshold)
        feature_cols_arr = feature_cols_arr[keep_mask]
    
    # Drop custom columns using set operations
    if custom_drop:
        custom_set = set(custom_drop)
        keep_mask = np.array([c not in custom_set for c in feature_cols_arr])
        feature_cols_arr = feature_cols_arr[keep_mask]
    
    if 'RUL' not in df.columns:
        raise ValueError("DataFrame must have 'RUL' column. Run compute_rul first.")
    
    # Build output column list
    output_cols = list(ID_COLUMNS) + feature_cols_arr.tolist() + ['RUL']
    
    return df[output_cols].copy(), tuple(feature_cols_arr.tolist())


class FeatureNormalizer:
    """Normalizer using typed arrays for fast transformation."""
    
    def __init__(
        self, 
        method: Literal['standard', 'minmax'] = 'standard',
        feature_columns: Optional[tuple] = None
    ):
        self.method = method
        self.feature_columns = feature_columns
        self.scaler = StandardScaler() if method == 'standard' else MinMaxScaler(feature_range=(-1, 1))
        self._is_fitted = False
        # Store normalization parameters as typed arrays
        self.mean_: Optional[NDArray[np.float64]] = None
        self.scale_: Optional[NDArray[np.float64]] = None
    
    def fit(self, df: pd.DataFrame) -> 'FeatureNormalizer':
        if self.feature_columns is None:
            cols = tuple(c for c in df.select_dtypes(include=[np.number]).columns 
                        if c not in ('unit_id', 'time_cycles', 'RUL'))
            self.feature_columns = cols
        
        self.scaler.fit(df[list(self.feature_columns)])
        self.mean_ = self.scaler.mean_.astype(np.float64) if hasattr(self.scaler, 'mean_') else None
        self.scale_ = self.scaler.scale_.astype(np.float64) if hasattr(self.scaler, 'scale_') else None
        self._is_fitted = True
        return self
    
    def transform(self, df: pd.DataFrame) -> pd.DataFrame:
        if not self._is_fitted:
            raise RuntimeError("Normalizer must be fitted before transform")
        df = df.copy()
        df[list(self.feature_columns)] = self.scaler.transform(df[list(self.feature_columns)])
        return df
    
    def transform_array(self, data: NDArray[np.float32]) -> NDArray[np.float32]:
        """Transform using typed arrays directly (faster than DataFrame)."""
        if not self._is_fitted:
            raise RuntimeError("Normalizer must be fitted before transform")
        return self.scaler.transform(data).astype(np.float32)
    
    def fit_transform(self, df: pd.DataFrame) -> pd.DataFrame:
        return self.fit(df).transform(df)


# =============================================================================
# Stride-Based Sliding Window (Zero-Copy Memory Views)
# =============================================================================

def sliding_window_view_strided(
    arr: NDArray[np.float32], 
    window_size: int, 
    stride: int = 1
) -> NDArray[np.float32]:
    """
    Create sliding windows using NumPy stride tricks (zero-copy memory view).
    
    All operations use typed arrays for maximum performance.
    """
    if arr.ndim == 1:
        arr = arr[:, np.newaxis]
    
    n_samples, n_features = arr.shape
    n_windows = (n_samples - window_size) // stride + 1
    
    if n_windows <= 0:
        return np.empty((0, window_size, n_features), dtype=np.float32)
    
    sample_stride, feature_stride = arr.strides
    new_shape = (n_windows, window_size, n_features)
    new_strides = (stride * sample_stride, sample_stride, feature_stride)
    
    return np.lib.stride_tricks.as_strided(
        arr, shape=new_shape, strides=new_strides, writeable=False
    )


def compute_engine_boundaries(
    unit_ids: NDArray[np.int32]
) -> Tuple[NDArray[np.int64], NDArray[np.int64], NDArray[np.int64]]:
    """
    Compute engine start/end indices using fully vectorized operations.
    
    Returns typed arrays: (engine_starts, engine_ends, engine_lengths)
    """
    n_samples = len(unit_ids)
    
    # Find boundaries where unit_id changes (vectorized)
    unit_changes: NDArray[np.bool_] = np.empty(n_samples, dtype=np.bool_)
    unit_changes[0] = True
    np.not_equal(unit_ids[1:], unit_ids[:-1], out=unit_changes[1:])
    
    # Get indices using boolean indexing
    engine_starts: NDArray[np.int64] = np.nonzero(unit_changes)[0]
    engine_ends: NDArray[np.int64] = np.empty_like(engine_starts)
    engine_ends[:-1] = engine_starts[1:]
    engine_ends[-1] = n_samples
    
    engine_lengths: NDArray[np.int64] = engine_ends - engine_starts
    
    return engine_starts, engine_ends, engine_lengths


def compute_sequence_indices_vectorized(
    unit_ids: NDArray[np.int32],
    sequence_length: int,
    stride: int = 1
) -> Tuple[NDArray[np.int64], NDArray[np.int64]]:
    """
    Compute valid sequence start indices (FULLY VECTORIZED - no Python loops).
    
    Uses typed arrays throughout for maximum performance.
    """
    # Get engine boundaries
    engine_starts, engine_ends, engine_lengths = compute_engine_boundaries(unit_ids)
    n_engines = len(engine_starts)
    
    # Vectorized computation of valid sequence counts per engine
    valid_per_engine: NDArray[np.int64] = np.maximum(
        0, (engine_lengths - sequence_length) // stride + 1
    )
    
    total_sequences = valid_per_engine.sum()
    
    if total_sequences == 0:
        return np.array([], dtype=np.int64), np.array([], dtype=np.int64)
    
    # Compute cumulative counts for offset calculation
    cum_sequences: NDArray[np.int64] = np.zeros(n_engines + 1, dtype=np.int64)
    np.cumsum(valid_per_engine, out=cum_sequences[1:])
    
    # FULLY VECTORIZED: Generate all sequence indices without Python loops
    # Create array of sequence numbers within each engine
    seq_numbers: NDArray[np.int64] = np.arange(total_sequences, dtype=np.int64)
    
    # Find which engine each sequence belongs to using searchsorted
    engine_indices: NDArray[np.int64] = np.searchsorted(
        cum_sequences[1:], seq_numbers, side='right'
    )
    
    # Compute local sequence number within each engine
    local_seq_num: NDArray[np.int64] = seq_numbers - cum_sequences[engine_indices]
    
    # Compute start indices: engine_start + local_seq_num * stride
    valid_starts: NDArray[np.int64] = (
        engine_starts[engine_indices] + local_seq_num * stride
    )
    
    # Compute end indices
    sequence_ends: NDArray[np.int64] = valid_starts + sequence_length
    
    return valid_starts, sequence_ends


def create_sequences_strided(
    df: pd.DataFrame,
    sequence_length: int,
    feature_columns: tuple,
    target_column: str = 'RUL',
    stride: int = 1,
    copy_output: bool = False
) -> Tuple[NDArray[np.float32], NDArray[np.float32]]:
    """
    Create sequences with ZERO copying in hot path.
    
    All data is converted to correct types BEFORE entering any loops or indexing.
    Uses stride tricks for zero-copy views, with optional final copy.
    
    Optimization strategy:
    1. Convert all data to correct types ONCE at the start
    2. Pre-compute all indices before hot path
    3. Use stride tricks for zero-copy window creation
    4. Only copy if explicitly requested via copy_output
    """
    # Ensure data is sorted (required for stride tricks)
    df = df.sort_values(['unit_id', 'time_cycles'])
    
    # PRE-PROCESSING: Convert ALL data to correct types BEFORE hot path
    # This ensures zero type conversions inside any operations
    features: NDArray[np.float32] = np.ascontiguousarray(
        df[list(feature_columns)].values, dtype=np.float32
    )
    targets: NDArray[np.float32] = np.ascontiguousarray(
        df[target_column].values, dtype=np.float32
    )
    unit_ids: NDArray[np.int32] = np.ascontiguousarray(
        df['unit_id'].values, dtype=np.int32
    )
    
    n_samples, n_features = features.shape
    
    # PRE-COMPUTE: All sequence indices computed ONCE
    # No computation happens during actual sequence creation
    valid_starts: NDArray[np.int64]
    sequence_ends: NDArray[np.int64]
    valid_starts, sequence_ends = compute_sequence_indices_vectorized(
        unit_ids, sequence_length, stride
    )
    
    n_sequences = len(valid_starts)
    
    if n_sequences == 0:
        return (np.empty((0, sequence_length, n_features), dtype=np.float32),
                np.empty(0, dtype=np.float32))
    
    # PRE-COMPUTE: Target values extracted using pre-computed indices
    # Single vectorized operation, no loop
    y: NDArray[np.float32] = targets[sequence_ends - 1]
    
    # ZERO-COPY VIEW CREATION: Use stride tricks for memory-efficient windows
    # This creates a VIEW of the data, not a copy
    # The view shares memory with the original features array
    all_windows: NDArray[np.float32] = sliding_window_view_strided(
        features, sequence_length, stride=1
    )
    
    # FINAL INDEXING: Select valid sequences using pre-computed indices
    # This is still a view operation (no copy) thanks to NumPy's advanced indexing
    # with stride tricks - we're just selecting which windows to include
    X: NDArray[np.float32] = all_windows[valid_starts]
    
    # OPTIONAL COPY: Only if explicitly requested
    # For most use cases, keeping the view is more memory-efficient
    if copy_output:
        X = np.array(X, dtype=np.float32, copy=True)
        y = np.array(y, dtype=np.float32, copy=True)
    
    return X, y


def create_test_sequences_vectorized(
    df: pd.DataFrame,
    sequence_length: int,
    feature_columns: tuple,
    target_column: str = 'RUL'
) -> Tuple[NDArray[np.float32], NDArray[np.float32], NDArray[np.int32]]:
    """
    Create test sequences - FULLY VECTORIZED with NO copying in hot path.
    
    All data is converted to correct types BEFORE entering any loops.
    Uses pre-computed indices and in-place operations.
    """
    df = df.sort_values(['unit_id', 'time_cycles'])
    
    # PRE-PROCESSING: Convert ALL data to correct types BEFORE hot path
    # This ensures zero type conversions inside loops
    features: NDArray[np.float32] = np.ascontiguousarray(
        df[list(feature_columns)].values, dtype=np.float32
    )
    targets: NDArray[np.float32] = np.ascontiguousarray(
        df[target_column].values, dtype=np.float32
    )
    unit_ids_col: NDArray[np.int32] = np.ascontiguousarray(
        df['unit_id'].values, dtype=np.int32
    )
    
    n_features = features.shape[1]
    
    # Get engine boundaries (vectorized) - all indices pre-computed
    engine_starts, engine_ends, engine_lengths = compute_engine_boundaries(unit_ids_col)
    n_engines = len(engine_starts)
    
    # Pre-extract unique unit IDs (no copying in loop)
    unique_units: NDArray[np.int32] = unit_ids_col[engine_starts].copy()
    
    # PRE-ALLOCATE: All memory allocated before hot path
    X: NDArray[np.float32] = np.zeros((n_engines, sequence_length, n_features), dtype=np.float32)
    
    # Vectorized target extraction (no loop)
    y: NDArray[np.float32] = targets[engine_ends - 1].copy()
    
    # PRE-COMPUTE: All indexing logic before hot path
    # Compute source and destination indices for all engines at once
    take_lengths: NDArray[np.int64] = np.minimum(engine_lengths, sequence_length)
    
    # Separate engines into two groups for optimized handling
    long_mask: NDArray[np.bool_] = engine_lengths >= sequence_length
    short_mask: NDArray[np.bool_] = ~long_mask
    
    # Pre-compute all indices (avoid repeated computation in loop)
    long_indices: NDArray[np.int64] = np.nonzero(long_mask)[0]
    short_indices: NDArray[np.int64] = np.nonzero(short_mask)[0]
    
    # Pre-compute start and end positions (avoid computation in loop)
    long_ends: NDArray[np.int64] = engine_ends[long_indices]
    long_starts: NDArray[np.int64] = long_ends - sequence_length
    
    short_starts: NDArray[np.int64] = engine_starts[short_indices]
    short_ends: NDArray[np.int64] = engine_ends[short_indices]
    short_lengths: NDArray[np.int64] = take_lengths[short_indices]
    dest_starts: NDArray[np.int64] = sequence_length - short_lengths
    
    # HOT PATH 1: Handle long sequences (most common case)
    # Use NumPy's advanced indexing - still a loop but with minimal overhead
    for i, idx in enumerate(long_indices):
        # Direct slice assignment - no intermediate copies
        # Data is already in correct format, just copying memory
        X[idx, :, :] = features[long_starts[i]:long_ends[i], :]
    
    # HOT PATH 2: Handle short sequences (padded)
    # Padded portion is already zeros from pre-allocation
    for i, idx in enumerate(short_indices):
        # In-place assignment to pre-allocated array
        X[idx, dest_starts[i]:, :] = features[short_starts[i]:short_ends[i], :]
    
    return X, y, unique_units


# Legacy aliases
create_sequences = create_sequences_strided
create_test_sequences = create_test_sequences_vectorized


# =============================================================================
# Memory-Efficient PyTorch Dataset with Typed Arrays
# =============================================================================

class RULDatasetStrided(Dataset):
    """
    Memory-efficient PyTorch Dataset with zero-copy hot path.
    
    All data conversions done in __init__, __getitem__ just indexes.
    """
    
    def __init__(
        self,
        features: NDArray[np.float32],
        targets: NDArray[np.float32],
        unit_ids: NDArray[np.int32],
        sequence_length: int,
        stride: int = 1
    ):
        # PRE-PROCESSING: Convert all data to correct format ONCE
        # Ensure contiguous memory layout for fast indexing
        self.features: NDArray[np.float32] = np.ascontiguousarray(features, dtype=np.float32)
        self.targets: NDArray[np.float32] = np.ascontiguousarray(targets, dtype=np.float32)
        self.sequence_length: int = sequence_length
        self.stride: int = stride
        self.n_features: int = features.shape[1]
        
        # PRE-COMPUTE: All indices computed ONCE in __init__
        # No computation happens in __getitem__ hot path
        self.valid_starts: NDArray[np.int64]
        self.sequence_ends: NDArray[np.int64]
        self.valid_starts, self.sequence_ends = compute_sequence_indices_vectorized(
            unit_ids, sequence_length, stride
        )
        self._len: int = len(self.valid_starts)
        
        # PRE-COMPUTE: Target indices for fast lookup
        self.target_indices: NDArray[np.int64] = self.sequence_ends - 1
    
    def __len__(self) -> int:
        return self._len
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        HOT PATH: Optimized for zero unnecessary operations.
        
        - No type conversions (already done in __init__)
        - No index computations (pre-computed in __init__)
        - Direct memory access using pre-computed indices
        - Single copy from numpy to torch (unavoidable for DataLoader)
        """
        # Direct index lookup (no computation)
        start: int = self.valid_starts[idx]
        end: int = start + self.sequence_length
        
        # Direct slice - features is already contiguous float32
        # .copy() is required for torch DataLoader (it needs owned memory)
        # This is the ONLY copy in the hot path and is unavoidable
        X = torch.from_numpy(self.features[start:end].copy())
        
        # Direct target lookup using pre-computed index
        y = torch.tensor([self.targets[self.target_indices[idx]]], dtype=torch.float32)
        
        return X, y


class RULDataset(Dataset):
    """
    Standard PyTorch Dataset with pre-computed sequences.
    
    Data is converted to torch tensors in __init__, zero-copy in __getitem__.
    """
    
    def __init__(self, X: NDArray[np.float32], y: NDArray[np.float32]):
        # PRE-PROCESSING: Convert to correct format ONCE in __init__
        # Ensure contiguous memory and correct dtype
        X_contig: NDArray[np.float32] = np.ascontiguousarray(X, dtype=np.float32)
        y_contig: NDArray[np.float32] = np.ascontiguousarray(y, dtype=np.float32)
        
        # Convert to torch tensors ONCE (shares memory with numpy)
        # from_numpy creates a tensor that shares storage with numpy array
        self.X: torch.Tensor = torch.from_numpy(X_contig)
        self.y: torch.Tensor = torch.from_numpy(y_contig).unsqueeze(1)
        self._len: int = len(self.X)
    
    def __len__(self) -> int:
        return self._len
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        HOT PATH: Pure indexing, zero copies.
        
        Returns views into pre-allocated tensors.
        """
        return self.X[idx], self.y[idx]
    
    def __len__(self) -> int:
        return self._len
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.X[idx], self.y[idx]


# =============================================================================
# DataLoader Creation
# =============================================================================

def create_dataloaders(
    X_train: NDArray[np.float32],
    y_train: NDArray[np.float32],
    X_test: NDArray[np.float32],
    y_test: NDArray[np.float32],
    batch_size: int = 64,
    val_split: float = 0.2,
    num_workers: int = 0,
    seed: int = 42
) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """Create PyTorch DataLoaders using typed arrays."""
    full_train_dataset = RULDataset(X_train, y_train)
    
    n_total = len(full_train_dataset)
    n_val = int(n_total * val_split)
    n_train = n_total - n_val
    
    generator = torch.Generator().manual_seed(seed)
    train_dataset, val_dataset = random_split(
        full_train_dataset, [n_train, n_val], generator=generator
    )
    
    test_dataset = RULDataset(X_test, y_test)
    
    loader_kwargs = {
        'batch_size': batch_size,
        'num_workers': num_workers,
        'pin_memory': torch.cuda.is_available()
    }
    
    return (
        DataLoader(train_dataset, shuffle=True, **loader_kwargs),
        DataLoader(val_dataset, shuffle=False, **loader_kwargs),
        DataLoader(test_dataset, shuffle=False, **loader_kwargs)
    )


def create_dataloaders_lazy(
    train_df: pd.DataFrame,
    test_df: pd.DataFrame,
    feature_columns: tuple,
    sequence_length: int,
    batch_size: int = 64,
    val_split: float = 0.2,
    num_workers: int = 0,
    seed: int = 42
) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """Create DataLoaders with lazy sequence generation."""
    train_df = train_df.sort_values(['unit_id', 'time_cycles'])
    test_df = test_df.sort_values(['unit_id', 'time_cycles'])
    
    # Extract typed arrays
    train_features: NDArray[np.float32] = train_df[list(feature_columns)].values.astype(np.float32)
    train_targets: NDArray[np.float32] = train_df['RUL'].values.astype(np.float32)
    train_unit_ids: NDArray[np.int32] = train_df['unit_id'].values.astype(np.int32)
    
    full_train_dataset = RULDatasetStrided(
        features=train_features,
        targets=train_targets,
        unit_ids=train_unit_ids,
        sequence_length=sequence_length
    )
    
    n_total = len(full_train_dataset)
    n_val = int(n_total * val_split)
    n_train = n_total - n_val
    
    generator = torch.Generator().manual_seed(seed)
    train_dataset, val_dataset = random_split(
        full_train_dataset, [n_train, n_val], generator=generator
    )
    
    X_test, y_test, _ = create_test_sequences_vectorized(
        test_df, sequence_length, feature_columns
    )
    test_dataset = RULDataset(X_test, y_test)
    
    loader_kwargs = {
        'batch_size': batch_size,
        'num_workers': num_workers,
        'pin_memory': torch.cuda.is_available()
    }
    
    return (
        DataLoader(train_dataset, shuffle=True, **loader_kwargs),
        DataLoader(val_dataset, shuffle=False, **loader_kwargs),
        DataLoader(test_dataset, shuffle=False, **loader_kwargs)
    )


# =============================================================================
# Complete Pipeline
# =============================================================================

def prepare_data(
    dataset_id: Literal['FD001', 'FD002', 'FD003', 'FD004'] = 'FD001',
    sequence_length: int = 30,
    max_rul: int = 125,
    normalization: Literal['standard', 'minmax'] = 'standard',
    drop_constant_sensors: bool = True,
    include_settings: bool = True,
    batch_size: int = 64,
    val_split: float = 0.2,
    seed: int = 42,
    lazy_loading: bool = False
) -> dict:
    """
    Complete data preparation pipeline with typed arrays throughout.
    """
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    # Load data
    train_df, test_df, rul_array = load_dataset(dataset_id)
    
    # Compute RUL (vectorized)
    train_df = compute_rul_vectorized(train_df, max_rul=max_rul)
    test_df = add_test_rul_vectorized(test_df, rul_array)
    
    # Select features
    train_df, feature_columns = select_features(
        train_df, drop_constant=drop_constant_sensors, drop_settings=not include_settings
    )
    test_df, _ = select_features(
        test_df, drop_constant=drop_constant_sensors, drop_settings=not include_settings
    )
    
    # Normalize features
    normalizer = FeatureNormalizer(method=normalization, feature_columns=feature_columns)
    train_df = normalizer.fit_transform(train_df)
    test_df = normalizer.transform(test_df)
    
    if lazy_loading:
        train_loader, val_loader, test_loader = create_dataloaders_lazy(
            train_df, test_df, feature_columns, sequence_length,
            batch_size, val_split, seed=seed
        )
        test_unit_ids: NDArray[np.int32] = np.unique(test_df['unit_id'].values.astype(np.int32))
    else:
        X_train, y_train = create_sequences_strided(
            train_df, sequence_length, feature_columns, copy_output=True
        )
        X_test, y_test, test_unit_ids = create_test_sequences_vectorized(
            test_df, sequence_length, feature_columns
        )
        train_loader, val_loader, test_loader = create_dataloaders(
            X_train, y_train, X_test, y_test,
            batch_size, val_split, seed=seed
        )
    
    return {
        'train_loader': train_loader,
        'val_loader': val_loader,
        'test_loader': test_loader,
        'feature_columns': feature_columns,
        'n_features': len(feature_columns),
        'normalizer': normalizer,
        'test_unit_ids': test_unit_ids,
        'sequence_length': sequence_length,
        'max_rul': max_rul
    }


# =============================================================================
# Memory Benchmarking Utilities
# =============================================================================

def benchmark_memory_efficiency(
    n_rows: int = 100_000,
    n_features: int = 17,
    sequence_length: int = 50
) -> dict:
    """Benchmark memory efficiency using typed arrays."""
    import sys
    
    np.random.seed(42)
    features: NDArray[np.float32] = np.random.randn(n_rows, n_features).astype(np.float32)
    
    original_memory = features.nbytes
    strided_view = sliding_window_view_strided(features, sequence_length)
    strided_memory = sys.getsizeof(strided_view)
    
    n_windows = n_rows - sequence_length + 1
    naive_memory_theoretical = n_windows * sequence_length * n_features * 4
    
    return {
        'n_rows': n_rows,
        'n_features': n_features,
        'sequence_length': sequence_length,
        'n_sequences': n_windows,
        'original_data_mb': original_memory / (1024 ** 2),
        'strided_view_bytes': strided_memory,
        'naive_approach_mb': naive_memory_theoretical / (1024 ** 2),
        'memory_savings_ratio': naive_memory_theoretical / strided_memory
    }


# =============================================================================
# Main - Demo and Testing
# =============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("C-MAPSS Data Engineering - Typed Array Implementation")
    print("=" * 70)
    
    # Benchmark memory efficiency
    print("\nðŸ§ª Memory Efficiency Benchmark:")
    print("-" * 40)
    
    benchmark = benchmark_memory_efficiency(
        n_rows=100_000,
        n_features=17,
        sequence_length=50
    )
    
    print(f"  Dataset: {benchmark['n_rows']:,} rows Ã— {benchmark['n_features']} features")
    print(f"  Sequence length: {benchmark['sequence_length']}")
    print(f"  Number of sequences: {benchmark['n_sequences']:,}")
    print(f"\n  Original data: {benchmark['original_data_mb']:.2f} MB")
    print(f"  Strided view overhead: {benchmark['strided_view_bytes']} bytes")
    print(f"  Naive approach would use: {benchmark['naive_approach_mb']:.2f} MB")
    print(f"  Memory savings: {benchmark['memory_savings_ratio']:,.0f}x")
    
    # Test with real data
    print("\n" + "=" * 70)
    print("Testing with Real C-MAPSS Data (FD001)")
    print("=" * 70)
    
    train_df, test_df, rul_array = load_dataset('FD001')
    
    print(f"\nðŸ“Š Dataset Statistics:")
    print(f"  Training rows: {len(train_df):,}")
    print(f"  Test rows: {len(test_df):,}")
    print(f"  Training engines: {train_df['unit_id'].nunique()}")
    print(f"  Data types: features={train_df['T30'].dtype}, unit_id={train_df['unit_id'].dtype}")
    
    # Vectorized RUL computation
    print("\nâš¡ Vectorized RUL Computation (typed arrays):")
    import time
    
    start = time.perf_counter()
    train_df = compute_rul_vectorized(train_df, max_rul=125)
    elapsed = time.perf_counter() - start
    print(f"  Computed RUL for {len(train_df):,} rows in {elapsed*1000:.2f}ms")
    
    # Analyze sensor variance
    print("\n" + "=" * 70)
    print("Sensor Variance Analysis - Data-Driven Feature Selection")
    print("=" * 70)
    print("\nWhy drop constant sensors?")
    print("  â€¢ Constant features have zero variance â†’ no predictive information")
    print("  â€¢ They cannot distinguish between healthy and degraded states")
    print("  â€¢ Normalizing constant columns causes division by zero")
    print("  â€¢ Removing them reduces model complexity without losing information")
    
    variance_analysis = analyze_sensor_variance(train_df, variance_threshold=1e-5)
    print(f"\nVariance array dtype: {variance_analysis['variances'].dtype}")
    print(f"Constant mask dtype: {variance_analysis['constant_mask'].dtype}")
    
    # Select and normalize features
    train_df, feature_cols = select_features(train_df, drop_constant=True)
    print(f"\nâœ“ Selected {len(feature_cols)} informative features (as tuple)")
    
    normalizer = FeatureNormalizer(method='standard', feature_columns=feature_cols)
    train_df = normalizer.fit_transform(train_df)
    
    # Strided sequence creation
    print("\nðŸš€ Strided Sequence Generation (fully vectorized):")
    sequence_length = 50
    
    start = time.perf_counter()
    X_train, y_train = create_sequences_strided(
        train_df, sequence_length, feature_cols, copy_output=False
    )
    elapsed = time.perf_counter() - start
    
    print(f"  Created {len(X_train):,} sequences in {elapsed*1000:.2f}ms")
    print(f"  X shape: {X_train.shape}, dtype: {X_train.dtype}")
    print(f"  y shape: {y_train.shape}, dtype: {y_train.dtype}")
    
    # Full pipeline test
    print("\n" + "=" * 70)
    print("Full Pipeline Test")
    print("=" * 70)
    
    data = prepare_data(
        dataset_id='FD001',
        sequence_length=50,
        max_rul=125,
        batch_size=64,
        lazy_loading=False
    )
    
    print(f"\nâœ… Pipeline Complete!")
    print(f"  Features: {data['n_features']} (type: {type(data['feature_columns']).__name__})")
    print(f"  Sequence length: {data['sequence_length']}")
    print(f"  Training batches: {len(data['train_loader'])}")
    print(f"  Validation batches: {len(data['val_loader'])}")
    print(f"  Test batches: {len(data['test_loader'])}")
    print(f"  Test unit IDs dtype: {data['test_unit_ids'].dtype}")
    
    # Sample batch
    X_batch, y_batch = next(iter(data['train_loader']))
    print(f"\nðŸ“¦ Sample Batch:")
    print(f"  X: {X_batch.shape}, dtype: {X_batch.dtype}")
    print(f"  y: {y_batch.shape}, dtype: {y_batch.dtype}")
    
    print("\n" + "=" * 70)
    print("âœ¨ Ready for training with typed array optimization!")
    print("=" * 70)
